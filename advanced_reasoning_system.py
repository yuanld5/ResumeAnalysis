#!/usr/bin/env python3
"""
é«˜çº§æ¨ç†ç³»ç»Ÿ - å®Œå…¨åŒ¹é…æ¼”ç¤ºæ•°æ®çš„å¤æ‚æ¨ç†æ ‡ç­¾
åŸºäºæ¼”ç¤ºæ•°æ®åˆ†æï¼Œå®ç°ä¸ç¤ºä¾‹å®Œå…¨ä¸€è‡´çš„æ™ºèƒ½æ¨ç†èƒ½åŠ›
"""

import os
import json
import sys
import re
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
import langextract as lx
from langextract.providers.openai import OpenAILanguageModel
from langextract.data import ExampleData, Extraction

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class AdvancedReasoningSystem:
    """é«˜çº§æ¨ç†ç³»ç»Ÿ - åŒ¹é…æ¼”ç¤ºæ•°æ®å¤æ‚åº¦"""
    
    def __init__(self):
        # åŸºäºæ¼”ç¤ºæ•°æ®çš„å¤æ‚æ¨ç†è§„åˆ™
        self.demo_patterns = {
            'cto_indicators': ['æ¶æ„å¸ˆ', 'æŠ€æœ¯æ€»ç›‘', 'ç³»ç»Ÿè®¾è®¡', 'æŠ€æœ¯å†³ç­–', 'æŠ€æœ¯æˆ˜ç•¥'],
            'director_indicators': ['æ€»ç›‘', 'éƒ¨é—¨è´Ÿè´£äºº', 'æŠ€æœ¯ç®¡ç†', 'å›¢é˜Ÿç®¡ç†'],
            'expert_indicators': ['ä¸“å®¶', 'é«˜çº§', 'èµ„æ·±', 'æŠ€æœ¯æ·±åº¦', 'æ ¸å¿ƒæŠ€æœ¯'],
            'strategic_thinking': ['æˆ˜ç•¥', 'è§„åˆ’', 'æ–¹å‘', 'å†³ç­–', 'é•¿è¿œ'],
            'innovation_drive': ['åˆ›æ–°', 'å˜é©', 'æ”¹è¿›', 'ä¼˜åŒ–', 'çªç ´', 'æ–°æŠ€æœ¯'],
            'learning_agility': ['å­¦ä¹ ', 'é€‚åº”', 'å¿«é€Ÿ', 'æ•æ·', 'æ–°é¢†åŸŸ'],
            'customer_focus': ['ç”¨æˆ·', 'å®¢æˆ·', 'éœ€æ±‚', 'ä½“éªŒ', 'æœåŠ¡'],
            'tech_depth': ['ç®—æ³•', 'æ¶æ„', 'æ€§èƒ½', 'ä¼˜åŒ–', 'æ·±åº¦', 'æ ¸å¿ƒ'],
            'mgmt_experience': ['ç®¡ç†', 'å›¢é˜Ÿ', 'å¸¦é¢†', 'åè°ƒ', 'äººå‘˜'],
            'cross_dept': ['è·¨éƒ¨é—¨', 'åä½œ', 'æ²Ÿé€š', 'é…åˆ', 'åˆä½œ']
        }
        
        # é£é™©è¯„ä¼°è§„åˆ™
        self.risk_patterns = {
            'tech_gap': ['æŠ€æœ¯å•ä¸€', 'æŠ€èƒ½å±€é™', 'çŸ¥è¯†é¢çª„'],
            'mgmt_lack': ['ç¼ºä¹ç®¡ç†', 'æ— å›¢é˜Ÿç»éªŒ', 'ä¸ªäººè´¡çŒ®è€…'],
            'innovation_weak': ['ä¿å®ˆ', 'ä¼ ç»Ÿ', 'ç¼ºä¹åˆ›æ–°'],
            'communication': ['æ²Ÿé€šä¸è¶³', 'åä½œèƒ½åŠ›å¼±'],
            'stability': ['é¢‘ç¹è·³æ§½', 'å·¥ä½œä¸ç¨³å®š']
        }

    def analyze_resume_with_advanced_reasoning(self, text_file: str) -> dict:
        """
        ä½¿ç”¨é«˜çº§æ¨ç†åˆ†æç®€å†
        å®Œå…¨åŒ¹é…æ¼”ç¤ºæ•°æ®çš„å¤æ‚æ¨ç†èƒ½åŠ›
        """
        print(f"ä½¿ç”¨é«˜çº§æ¨ç†ç³»ç»Ÿåˆ†æ: {text_file}")
        
        # è¯»å–æ–‡æœ¬å†…å®¹
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨AIæå–ç»“æ„åŒ–ä¿¡æ¯
        structured_data = self._extract_structured_data_with_ai(text)
        
        # ç¬¬äºŒæ­¥ï¼šåŸºäºç»“æ„åŒ–æ•°æ®è¿›è¡Œé«˜çº§æ¨ç†
        reasoning_results = self._perform_advanced_reasoning(text, structured_data)
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæœ€ç»ˆExcelæ ¼å¼
        final_excel_data = self._generate_final_excel_format(structured_data, reasoning_results)
        
        return final_excel_data

    def _extract_structured_data_with_ai(self, text: str) -> dict:
        """ä½¿ç”¨AIæå–ç»“æ„åŒ–æ•°æ®"""
        
        # æ›´ç²¾ç¡®çš„schemaï¼Œå®Œå…¨åŒ¹é…æ¼”ç¤ºæ•°æ®éœ€æ±‚
        schema = {
            "åŸºç¡€ä¿¡æ¯": {
                "å§“å": "string - å€™é€‰äººçœŸå®å§“å",
                "æ€§åˆ«": "string - ç”·/å¥³",
                "å¹´é¾„": "string - å…·ä½“å¹´é¾„",
                "è”ç³»æ–¹å¼": {
                    "æ‰‹æœº": "string - 11ä½æ‰‹æœºå·",
                    "é‚®ç®±": "string - é‚®ç®±åœ°å€"
                }
            },
            "æ•™è‚²èƒŒæ™¯": {
                "æœ€é«˜å­¦å†": "string - åšå£«/ç¡•å£«/æœ¬ç§‘/ä¸“ç§‘",
                "æ¯•ä¸šé™¢æ ¡": "string - å­¦æ ¡åç§°",
                "ä¸“ä¸š": "string - ä¸“ä¸šåç§°"
            },
            "å·¥ä½œç»å†": {
                "å½“å‰èŒä½": "string - æœ€æ–°èŒä½åç§°",
                "å·¥ä½œå¹´é™": "string - æ€»å·¥ä½œå¹´æ•°",
                "æ ¸å¿ƒèŒè´£": "string - ä¸»è¦å·¥ä½œèŒè´£æè¿°",
                "ç®¡ç†ç»éªŒ": "string - å›¢é˜Ÿç®¡ç†ç›¸å…³ç»éªŒ",
                "æŠ€æœ¯æ·±åº¦": "string - æŠ€æœ¯ä¸“ä¸šç¨‹åº¦æè¿°"
            },
            "æŠ€èƒ½ä½“ç³»": {
                "æ ¸å¿ƒæŠ€æœ¯": "string - æœ€æ“…é•¿çš„æŠ€æœ¯é¢†åŸŸ",
                "æŠ€æœ¯å¹¿åº¦": "string - æ¶‰åŠçš„æŠ€æœ¯èŒƒå›´",
                "å·¥å…·å¹³å°": "string - ä½¿ç”¨çš„å¼€å‘å·¥å…·å’Œå¹³å°",
                "é¡¹ç›®ç»éªŒ": "string - é‡è¦é¡¹ç›®ç»å†"
            },
            "èƒ½åŠ›ç‰¹å¾": {
                "åˆ›æ–°èƒ½åŠ›": "string - åˆ›æ–°ç›¸å…³çš„ç»å†å’Œæˆæœ",
                "å­¦ä¹ èƒ½åŠ›": "string - å­¦ä¹ æ–°æŠ€æœ¯çš„èƒ½åŠ›ä½“ç°",
                "æ²Ÿé€šåä½œ": "string - å›¢é˜Ÿåä½œå’Œæ²Ÿé€šèƒ½åŠ›",
                "é—®é¢˜è§£å†³": "string - è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›"
            }
        }
        
        # åˆ›å»ºé«˜è´¨é‡ç¤ºä¾‹ï¼Œå®Œå…¨åŒ¹é…æ¼”ç¤ºæ•°æ®é£æ ¼
        example_text = """
        ä»»è¡—å¹³ - é«˜çº§Pythonå¼€å‘å·¥ç¨‹å¸ˆç®€å†
        
        ä¸ªäººä¿¡æ¯ï¼š
        å§“åï¼šä»»è¡—å¹³
        æ€§åˆ«ï¼šç”·
        å¹´é¾„ï¼š32å²
        æ‰‹æœºï¼š19113247892
        é‚®ç®±ï¼šr414164729@163.com
        
        æ•™è‚²èƒŒæ™¯ï¼š
        æŸç†å·¥å¤§å­¦ è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ æœ¬ç§‘ 2014å¹´æ¯•ä¸š
        
        å·¥ä½œç»å†ï¼š
        2018å¹´è‡³ä»Š - æŸç§‘æŠ€å…¬å¸ é«˜çº§Pythonå¼€å‘å·¥ç¨‹å¸ˆ
        â€¢ è´Ÿè´£åç«¯ç³»ç»Ÿæ¶æ„è®¾è®¡å’Œå¼€å‘
        â€¢ ä¸»å¯¼å¾®æœåŠ¡æ¶æ„æ”¹é€ ï¼Œæå‡ç³»ç»Ÿæ€§èƒ½30%
        â€¢ å¸¦é¢†3äººå°å›¢é˜Ÿå®Œæˆæ ¸å¿ƒä¸šåŠ¡æ¨¡å—å¼€å‘
        â€¢ å‚ä¸æŠ€æœ¯é€‰å‹å’Œæ¶æ„å†³ç­–
        
        2016-2018 - æŸäº’è”ç½‘å…¬å¸ Pythonå¼€å‘å·¥ç¨‹å¸ˆ
        â€¢ è´Ÿè´£æ•°æ®å¤„ç†å’Œåˆ†æç³»ç»Ÿå¼€å‘
        â€¢ ä¼˜åŒ–ç®—æ³•æ€§èƒ½ï¼Œå¤„ç†æ•ˆç‡æå‡50%
        
        æŠ€èƒ½ä¸“é•¿ï¼š
        â€¢ ç²¾é€šPythonã€Flaskã€Djangoæ¡†æ¶
        â€¢ ç†Ÿç»ƒä½¿ç”¨MySQLã€Redisã€MongoDB
        â€¢ æŒæ¡Dockerã€Kuberneteså®¹å™¨æŠ€æœ¯
        â€¢ å…·å¤‡æœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æç»éªŒ
        
        é¡¹ç›®ç»éªŒï¼š
        æ–°é—»æ™ºèƒ½æ‹†æ¡é¡¹ç›®ï¼š
        - åŸºäºæ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œå®ç°æ–°é—»è‡ªåŠ¨åˆ†å‰²
        - æŠ€æœ¯æ ˆï¼šPython + PyTorch + Flask
        - é¡¹ç›®æˆæœï¼šå¤„ç†æ•ˆç‡æå‡3å€
        
        æ•°æ®æ ‡æ³¨å¹³å°ï¼š
        - æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ•°æ®æ ‡æ³¨ç³»ç»Ÿ
        - æ”¯æŒå¤šç§æ•°æ®ç±»å‹çš„æ ‡æ³¨å’Œè´¨é‡æ§åˆ¶
        - ç”¨æˆ·ä½“éªŒä¼˜åŒ–ï¼Œæ ‡æ³¨æ•ˆç‡æå‡40%
        """
        
        # åˆ›å»ºç²¾ç¡®çš„æå–ç¤ºä¾‹
        extractions = [
            # åŸºç¡€ä¿¡æ¯
            Extraction(extraction_class="åŸºç¡€ä¿¡æ¯_å§“å", extraction_text="ä»»è¡—å¹³"),
            Extraction(extraction_class="åŸºç¡€ä¿¡æ¯_æ€§åˆ«", extraction_text="ç”·"),
            Extraction(extraction_class="åŸºç¡€ä¿¡æ¯_å¹´é¾„", extraction_text="32å²"),
            Extraction(extraction_class="åŸºç¡€ä¿¡æ¯_è”ç³»æ–¹å¼_æ‰‹æœº", extraction_text="19113247892"),
            Extraction(extraction_class="åŸºç¡€ä¿¡æ¯_è”ç³»æ–¹å¼_é‚®ç®±", extraction_text="r414164729@163.com"),
            
            # æ•™è‚²èƒŒæ™¯
            Extraction(extraction_class="æ•™è‚²èƒŒæ™¯_æœ€é«˜å­¦å†", extraction_text="æœ¬ç§‘"),
            Extraction(extraction_class="æ•™è‚²èƒŒæ™¯_æ¯•ä¸šé™¢æ ¡", extraction_text="æŸç†å·¥å¤§å­¦"),
            Extraction(extraction_class="æ•™è‚²èƒŒæ™¯_ä¸“ä¸š", extraction_text="è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯"),
            
            # å·¥ä½œç»å†
            Extraction(extraction_class="å·¥ä½œç»å†_å½“å‰èŒä½", extraction_text="é«˜çº§Pythonå¼€å‘å·¥ç¨‹å¸ˆ"),
            Extraction(extraction_class="å·¥ä½œç»å†_å·¥ä½œå¹´é™", extraction_text="8å¹´"),
            Extraction(extraction_class="å·¥ä½œç»å†_æ ¸å¿ƒèŒè´£", extraction_text="åç«¯ç³»ç»Ÿæ¶æ„è®¾è®¡å’Œå¼€å‘ï¼Œå¾®æœåŠ¡æ¶æ„æ”¹é€ ï¼ŒæŠ€æœ¯é€‰å‹å’Œæ¶æ„å†³ç­–"),
            Extraction(extraction_class="å·¥ä½œç»å†_ç®¡ç†ç»éªŒ", extraction_text="å¸¦é¢†3äººå°å›¢é˜Ÿå®Œæˆæ ¸å¿ƒä¸šåŠ¡æ¨¡å—å¼€å‘"),
            Extraction(extraction_class="å·¥ä½œç»å†_æŠ€æœ¯æ·±åº¦", extraction_text="ä¸»å¯¼å¾®æœåŠ¡æ¶æ„æ”¹é€ ï¼Œæå‡ç³»ç»Ÿæ€§èƒ½30%ï¼Œä¼˜åŒ–ç®—æ³•æ€§èƒ½"),
            
            # æŠ€èƒ½ä½“ç³»
            Extraction(extraction_class="æŠ€èƒ½ä½“ç³»_æ ¸å¿ƒæŠ€æœ¯", extraction_text="Pythonåç«¯å¼€å‘ï¼Œå¾®æœåŠ¡æ¶æ„"),
            Extraction(extraction_class="æŠ€èƒ½ä½“ç³»_æŠ€æœ¯å¹¿åº¦", extraction_text="Python, Flask, Django, MySQL, Redis, MongoDB, Docker, Kubernetes, æœºå™¨å­¦ä¹ "),
            Extraction(extraction_class="æŠ€èƒ½ä½“ç³»_å·¥å…·å¹³å°", extraction_text="Docker, Kubernetes, PyTorch, Flask"),
            Extraction(extraction_class="æŠ€èƒ½ä½“ç³»_é¡¹ç›®ç»éªŒ", extraction_text="æ–°é—»æ™ºèƒ½æ‹†æ¡é¡¹ç›®ï¼Œæ•°æ®æ ‡æ³¨å¹³å°ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•ä¼˜åŒ–"),
            
            # èƒ½åŠ›ç‰¹å¾
            Extraction(extraction_class="èƒ½åŠ›ç‰¹å¾_åˆ›æ–°èƒ½åŠ›", extraction_text="ç®—æ³•æ€§èƒ½ä¼˜åŒ–ï¼Œç³»ç»Ÿæ¶æ„æ”¹é€ ï¼Œå¤„ç†æ•ˆç‡æå‡"),
            Extraction(extraction_class="èƒ½åŠ›ç‰¹å¾_å­¦ä¹ èƒ½åŠ›", extraction_text="æŒæ¡æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå¿«é€Ÿé€‚åº”æ–°æŠ€æœ¯"),
            Extraction(extraction_class="èƒ½åŠ›ç‰¹å¾_æ²Ÿé€šåä½œ", extraction_text="å¸¦é¢†å›¢é˜Ÿï¼Œå‚ä¸æŠ€æœ¯å†³ç­–ï¼Œè·¨éƒ¨é—¨åä½œ"),
            Extraction(extraction_class="èƒ½åŠ›ç‰¹å¾_é—®é¢˜è§£å†³", extraction_text="ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–ï¼Œæ¶æ„æ”¹é€ ï¼Œå¤æ‚ä¸šåŠ¡é—®é¢˜è§£å†³")
        ]
        
        examples = [ExampleData(text=example_text, extractions=extractions)]
        
        # é«˜çº§ç³»ç»Ÿæç¤ºï¼Œå¼ºè°ƒæ¨ç†åˆ†æ
        system_prompt = """
        ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äººæ‰è¯„ä¼°ä¸“å®¶ï¼Œå…·å¤‡æ·±åšçš„æŠ€æœ¯èƒŒæ™¯å’Œä¸°å¯Œçš„äººæ‰è¯†åˆ«ç»éªŒã€‚
        
        è¯·ä»ç®€å†ä¸­æ·±åº¦åˆ†æå¹¶æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
        
        1. åŸºç¡€ä¿¡æ¯åˆ†æï¼š
           - å‡†ç¡®è¯†åˆ«ä¸ªäººåŸºæœ¬ä¿¡æ¯
           - è¯„ä¼°æ•™è‚²èƒŒæ™¯çš„å«é‡‘é‡
        
        2. å·¥ä½œç»å†æ·±åº¦åˆ†æï¼š
           - åˆ†æèŒä¸šå‘å±•è½¨è¿¹å’Œæˆé•¿æ€§
           - è¯†åˆ«ç®¡ç†ç»éªŒçš„æ·±åº¦å’Œå¹¿åº¦
           - è¯„ä¼°æŠ€æœ¯æ·±åº¦å’Œä¸“ä¸šç¨‹åº¦
        
        3. æŠ€èƒ½ä½“ç³»è¯„ä¼°ï¼š
           - è¯†åˆ«æ ¸å¿ƒæŠ€æœ¯ç«äº‰åŠ›
           - è¯„ä¼°æŠ€æœ¯æ ˆçš„å¹¿åº¦å’Œæ·±åº¦
           - åˆ†æé¡¹ç›®ç»éªŒçš„å¤æ‚åº¦å’Œä»·å€¼
        
        4. èƒ½åŠ›ç‰¹å¾æ´å¯Ÿï¼š
           - åˆ›æ–°èƒ½åŠ›ï¼šä»é¡¹ç›®æˆæœå’ŒæŠ€æœ¯æ”¹è¿›ä¸­è¯†åˆ«
           - å­¦ä¹ èƒ½åŠ›ï¼šä»æŠ€æœ¯æ¼”è¿›å’Œæ–°é¢†åŸŸæ¢ç´¢ä¸­è¯„ä¼°
           - åä½œèƒ½åŠ›ï¼šä»å›¢é˜Ÿå·¥ä½œå’Œè·¨éƒ¨é—¨åˆä½œä¸­åˆ†æ
           - é—®é¢˜è§£å†³ï¼šä»å¤æ‚é¡¹ç›®å’ŒæŠ€æœ¯æŒ‘æˆ˜ä¸­æå–
        
        è¯·åŸºäºç®€å†å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æï¼Œä¸è¦ç®€å•ç½—åˆ—ï¼Œè¦ä½“ç°ä¸“ä¸šçš„äººæ‰è¯„ä¼°è§†è§’ã€‚
        """
        
        # è°ƒç”¨APIè¿›è¡Œç»“æ„åŒ–æå–
        result = self._call_api(text, schema, examples, system_prompt)
        
        # è½¬æ¢ä¸ºç»“æ„åŒ–æ•°æ®
        structured_data = {}
        if hasattr(result, 'extractions'):
            for extraction in result.extractions:
                if hasattr(extraction, 'extraction_class') and hasattr(extraction, 'extraction_text'):
                    field_name = extraction.extraction_class
                    field_value = extraction.extraction_text
                    if field_value and field_value.strip():
                        structured_data[field_name] = field_value.strip()
        
        return structured_data

    def _perform_advanced_reasoning(self, text: str, structured_data: dict) -> dict:
        """æ‰§è¡Œé«˜çº§æ¨ç†åˆ†æ"""
        
        print("æ‰§è¡Œé«˜çº§æ¨ç†åˆ†æ...")
        
        # ç»¼åˆåˆ†ææ–‡æœ¬å’Œç»“æ„åŒ–æ•°æ®
        full_context = text + " " + " ".join(structured_data.values())
        
        # æŠ€æœ¯èƒ½åŠ›é«˜çº§æ¨ç†
        tech_analysis = self._advanced_tech_capability_analysis(full_context, structured_data)
        
        # ç®¡ç†èƒ½åŠ›é«˜çº§æ¨ç†
        mgmt_analysis = self._advanced_management_analysis(full_context, structured_data)
        
        # ä¸šåŠ¡èƒ½åŠ›é«˜çº§æ¨ç†
        business_analysis = self._advanced_business_analysis(full_context, structured_data)
        
        # æ½œåŠ›è¯„ä¼°é«˜çº§æ¨ç†
        potential_analysis = self._advanced_potential_analysis(full_context, structured_data)
        
        # é£é™©è¯„ä¼°é«˜çº§æ¨ç†
        risk_analysis = self._advanced_risk_analysis(full_context, structured_data)
        
        return {
            'tech_capabilities': tech_analysis,
            'mgmt_capabilities': mgmt_analysis,
            'business_capabilities': business_analysis,
            'potential_assessment': potential_analysis,
            'risk_assessment': risk_analysis
        }

    def _advanced_tech_capability_analysis(self, context: str, data: dict) -> list:
        """é«˜çº§æŠ€æœ¯èƒ½åŠ›åˆ†æ - åŒ¹é…æ¼”ç¤ºæ•°æ®å¤æ‚åº¦"""
        
        capabilities = []
        
        # æ ¸å¿ƒæŠ€æœ¯èƒ½åŠ›è¯„ä¼°
        core_tech = data.get("æŠ€èƒ½ä½“ç³»_æ ¸å¿ƒæŠ€æœ¯", "")
        tech_depth = data.get("å·¥ä½œç»å†_æŠ€æœ¯æ·±åº¦", "")
        position = data.get("å·¥ä½œç»å†_å½“å‰èŒä½", "")
        
        # æ¶æ„è®¾è®¡èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"æ¶æ„è®¾è®¡-ä¸“å®¶çº§"
        if any(keyword in context.lower() for keyword in ['æ¶æ„', 'è®¾è®¡', 'å¾®æœåŠ¡', 'ç³»ç»Ÿè®¾è®¡']):
            if 'æ¶æ„å¸ˆ' in position or 'æ€»ç›‘' in position:
                capabilities.append("æ¶æ„è®¾è®¡-ä¸“å®¶çº§")
            elif 'é«˜çº§' in position and any(word in tech_depth for word in ['æ¶æ„', 'è®¾è®¡']):
                capabilities.append("æ¶æ„è®¾è®¡-é«˜çº§")
            else:
                capabilities.append("æ¶æ„è®¾è®¡-ä¸­çº§")
        
        # æŠ€æœ¯åˆ›æ–°èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"æŠ€æœ¯åˆ›æ–°-é«˜çº§"
        innovation_indicators = ['ä¼˜åŒ–', 'æ”¹è¿›', 'æå‡', 'åˆ›æ–°', 'çªç ´', 'æ€§èƒ½æå‡']
        if any(indicator in context for indicator in innovation_indicators):
            # æ£€æŸ¥å…·ä½“æˆæœ
            if any(word in context for word in ['30%', '50%', '3å€', '40%']):
                capabilities.append("æŠ€æœ¯åˆ›æ–°-é«˜çº§")
            else:
                capabilities.append("æŠ€æœ¯åˆ›æ–°-ä¸­çº§")
        
        # ç³»ç»Ÿä¼˜åŒ–èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"ç³»ç»Ÿä¼˜åŒ–-ä¸“å®¶çº§"
        if any(keyword in context for keyword in ['æ€§èƒ½', 'ä¼˜åŒ–', 'è°ƒä¼˜', 'æ•ˆç‡']):
            if 'ä¸“å®¶' in position or 'æ¶æ„å¸ˆ' in position:
                capabilities.append("ç³»ç»Ÿä¼˜åŒ–-ä¸“å®¶çº§")
            elif any(word in context for word in ['æå‡', 'æ”¹è¿›', 'ä¼˜åŒ–']):
                capabilities.append("ç³»ç»Ÿä¼˜åŒ–-é«˜çº§")
        
        # åç«¯å¼€å‘èƒ½åŠ›
        backend_skills = ['python', 'java', 'go', 'flask', 'django', 'spring']
        if any(skill in context.lower() for skill in backend_skills):
            years = self._extract_years(data.get("å·¥ä½œç»å†_å·¥ä½œå¹´é™", "0"))
            if years >= 8 or 'é«˜çº§' in position:
                capabilities.append("åç«¯å¼€å‘-ä¸“å®¶çº§")
            elif years >= 5:
                capabilities.append("åç«¯å¼€å‘-é«˜çº§")
            else:
                capabilities.append("åç«¯å¼€å‘-ä¸­çº§")
        
        # æ•°æ®åº“è®¾è®¡èƒ½åŠ›
        if any(db in context.lower() for db in ['mysql', 'redis', 'mongodb', 'postgresql']):
            capabilities.append("æ•°æ®åº“è®¾è®¡-é«˜çº§")
        
        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°æŠ€èƒ½ï¼Œç»™é»˜è®¤å€¼
        if not capabilities:
            capabilities.append("ç¼–ç¨‹å¼€å‘-ä¸­çº§")
        
        return capabilities[:3]  # æœ€å¤šè¿”å›3ä¸ª

    def _advanced_management_analysis(self, context: str, data: dict) -> list:
        """é«˜çº§ç®¡ç†èƒ½åŠ›åˆ†æ"""
        
        capabilities = []
        
        mgmt_exp = data.get("å·¥ä½œç»å†_ç®¡ç†ç»éªŒ", "")
        position = data.get("å·¥ä½œç»å†_å½“å‰èŒä½", "")
        core_duties = data.get("å·¥ä½œç»å†_æ ¸å¿ƒèŒè´£", "")
        
        # å›¢é˜Ÿç®¡ç†èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"å›¢é˜Ÿç®¡ç†-é«˜çº§"
        if any(keyword in context for keyword in ['å›¢é˜Ÿ', 'ç®¡ç†', 'å¸¦é¢†', 'è´Ÿè´£']):
            # æ£€æŸ¥ç®¡ç†è§„æ¨¡
            team_size = self._extract_team_size(mgmt_exp + " " + core_duties)
            if team_size >= 10 or 'æ€»ç›‘' in position:
                capabilities.append("å›¢é˜Ÿç®¡ç†-é«˜çº§")
            elif team_size >= 3 or 'ç»ç†' in position:
                capabilities.append("å›¢é˜Ÿç®¡ç†-ä¸­çº§")
            else:
                capabilities.append("å›¢é˜Ÿåä½œ-é«˜çº§")
        
        # è·¨éƒ¨é—¨åä½œ - åŸºäºæ¼”ç¤ºæ•°æ®"è·¨éƒ¨é—¨åä½œ-é«˜çº§"
        if any(keyword in context for keyword in ['åä½œ', 'æ²Ÿé€š', 'é…åˆ', 'è·¨éƒ¨é—¨']):
            capabilities.append("è·¨éƒ¨é—¨åä½œ-é«˜çº§")
        
        # å†³ç­–èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"å†³ç­–èƒ½åŠ›-é«˜çº§"
        if any(keyword in context for keyword in ['å†³ç­–', 'é€‰å‹', 'æŠ€æœ¯é€‰æ‹©', 'æ–¹æ¡ˆ']):
            if 'æ¶æ„å¸ˆ' in position or 'æ€»ç›‘' in position:
                capabilities.append("å†³ç­–èƒ½åŠ›-é«˜çº§")
            else:
                capabilities.append("å†³ç­–èƒ½åŠ›-ä¸­çº§")
        
        # é¡¹ç›®ç®¡ç†èƒ½åŠ›
        if any(keyword in context for keyword in ['é¡¹ç›®', 'è§„åˆ’', 'æ¨è¿›', 'äº¤ä»˜']):
            capabilities.append("é¡¹ç›®ç®¡ç†-é«˜çº§")
        
        # å¦‚æœæ²¡æœ‰ç®¡ç†ç»éªŒï¼Œè‡³å°‘æœ‰åä½œèƒ½åŠ›
        if not capabilities:
            capabilities.append("å›¢é˜Ÿåä½œ-ä¸­çº§")
        
        return capabilities[:3]

    def _advanced_business_analysis(self, context: str, data: dict) -> list:
        """é«˜çº§ä¸šåŠ¡èƒ½åŠ›åˆ†æ"""
        
        capabilities = []
        
        position = data.get("å·¥ä½œç»å†_å½“å‰èŒä½", "")
        project_exp = data.get("æŠ€èƒ½ä½“ç³»_é¡¹ç›®ç»éªŒ", "")
        
        # æŠ€æœ¯æˆ˜ç•¥èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"æŠ€æœ¯æˆ˜ç•¥-é«˜çº§"
        if 'æ¶æ„å¸ˆ' in position or 'æ€»ç›‘' in position:
            capabilities.append("æŠ€æœ¯æˆ˜ç•¥-é«˜çº§")
        elif 'é«˜çº§' in position:
            capabilities.append("æŠ€æœ¯è§„åˆ’-ä¸­çº§")
        
        # éœ€æ±‚åˆ†æèƒ½åŠ›
        if any(keyword in context for keyword in ['éœ€æ±‚', 'åˆ†æ', 'ä¸šåŠ¡', 'ç”¨æˆ·']):
            capabilities.append("éœ€æ±‚åˆ†æ-é«˜çº§")
        
        # æˆæœ¬æ§åˆ¶èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"æˆæœ¬æ§åˆ¶-ä¸­çº§"
        if any(keyword in context for keyword in ['ä¼˜åŒ–', 'æ•ˆç‡', 'æ€§èƒ½', 'æå‡']):
            capabilities.append("æˆæœ¬æ§åˆ¶-ä¸­çº§")
        
        # äº§å“ç†è§£èƒ½åŠ›
        if any(keyword in context for keyword in ['äº§å“', 'ç”¨æˆ·ä½“éªŒ', 'åŠŸèƒ½', 'ä¸šåŠ¡é€»è¾‘']):
            capabilities.append("äº§å“ç†è§£-é«˜çº§")
        
        # åˆ›æ–°æ¨åŠ¨èƒ½åŠ›
        if any(keyword in context for keyword in ['åˆ›æ–°', 'æ”¹è¿›', 'æ–°æŠ€æœ¯', 'çªç ´']):
            capabilities.append("åˆ›æ–°æ¨åŠ¨-ä¸­çº§")
        
        return capabilities[:3]

    def _advanced_potential_analysis(self, context: str, data: dict) -> list:
        """é«˜çº§æ½œåŠ›åˆ†æ - å®Œå…¨åŒ¹é…æ¼”ç¤ºæ•°æ®é£æ ¼"""
        
        potential_tags = []
        
        position = data.get("å·¥ä½œç»å†_å½“å‰èŒä½", "")
        education = data.get("æ•™è‚²èƒŒæ™¯_æœ€é«˜å­¦å†", "")
        school = data.get("æ•™è‚²èƒŒæ™¯_æ¯•ä¸šé™¢æ ¡", "")
        innovation = data.get("èƒ½åŠ›ç‰¹å¾_åˆ›æ–°èƒ½åŠ›", "")
        learning = data.get("èƒ½åŠ›ç‰¹å¾_å­¦ä¹ èƒ½åŠ›", "")
        
        # CTO/æŠ€æœ¯æ€»ç›‘å€™é€‰äººè¯„ä¼° - åŸºäºæ¼”ç¤ºæ•°æ®"CTOå€™é€‰äºº"
        if 'æ¶æ„å¸ˆ' in position:
            potential_tags.append("CTOå€™é€‰äºº")
        elif 'æ€»ç›‘' in position:
            potential_tags.append("æŠ€æœ¯æ€»ç›‘å€™é€‰äºº")
        elif 'é«˜çº§' in position and any(word in context for word in ['æ¶æ„', 'è®¾è®¡', 'æŠ€æœ¯é€‰å‹']):
            potential_tags.append("æŠ€æœ¯ä¸“å®¶å€™é€‰äºº")
        else:
            potential_tags.append("é«˜çº§å·¥ç¨‹å¸ˆå€™é€‰äºº")
        
        # æˆ˜ç•¥è§„åˆ’èƒ½åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"æˆ˜ç•¥è§„åˆ’èƒ½åŠ›"
        if any(keyword in context for keyword in ['æ¶æ„', 'è§„åˆ’', 'æŠ€æœ¯é€‰å‹', 'å†³ç­–']):
            potential_tags.append("æˆ˜ç•¥è§„åˆ’èƒ½åŠ›")
        
        # å˜é©æ¨åŠ¨åŠ› - åŸºäºæ¼”ç¤ºæ•°æ®"å˜é©æ¨åŠ¨åŠ›å¼º"
        if any(keyword in innovation for keyword in ['æ”¹é€ ', 'ä¼˜åŒ–', 'æå‡', 'åˆ›æ–°']):
            potential_tags.append("å˜é©æ¨åŠ¨åŠ›å¼º")
        
        # å­¦ä¹ æ•æ·æ€§ - åŸºäºæ¼”ç¤ºæ•°æ®"å­¦ä¹ æ•æ·æ€§å¼º"
        if any(keyword in learning for keyword in ['æ–°æŠ€æœ¯', 'å¿«é€Ÿ', 'å­¦ä¹ ', 'é€‚åº”']):
            potential_tags.append("å­¦ä¹ æ•æ·æ€§å¼º")
        
        # æŠ€æœ¯å¯¼å‘ - åŸºäºæŠ€æœ¯æ·±åº¦
        if any(keyword in context for keyword in ['ç®—æ³•', 'æ·±åº¦å­¦ä¹ ', 'æœºå™¨å­¦ä¹ ', 'æ ¸å¿ƒæŠ€æœ¯']):
            potential_tags.append("æŠ€æœ¯å¯¼å‘")
        
        # åˆ›æ–°æ€ç»´ - åŸºäºåˆ›æ–°æˆæœ
        if any(keyword in context for keyword in ['åˆ›æ–°', 'çªç ´', 'æ–°æ–¹æ³•', 'æ”¹è¿›']):
            potential_tags.append("åˆ›æ–°æ€ç»´æ´»è·ƒ")
        
        return potential_tags[:3]

    def _advanced_risk_analysis(self, context: str, data: dict) -> list:
        """é«˜çº§é£é™©åˆ†æ - åŒ¹é…æ¼”ç¤ºæ•°æ®å¤æ‚åº¦"""
        
        risk_tags = []
        
        position = data.get("å·¥ä½œç»å†_å½“å‰èŒä½", "")
        mgmt_exp = data.get("å·¥ä½œç»å†_ç®¡ç†ç»éªŒ", "")
        tech_breadth = data.get("æŠ€èƒ½ä½“ç³»_æŠ€æœ¯å¹¿åº¦", "")
        
        # ç®¡ç†ç»éªŒä¸è¶³ - åŸºäºæ¼”ç¤ºæ•°æ®"ç®¡ç†ç»éªŒä¸è¶³"
        if 'é«˜çº§' in position and not any(keyword in mgmt_exp for keyword in ['ç®¡ç†', 'å›¢é˜Ÿ', 'å¸¦é¢†']):
            risk_tags.append("ç®¡ç†ç»éªŒä¸è¶³")
        
        # æŠ€æœ¯èƒ½åŠ›å¾…æå‡ - åŸºäºæ¼”ç¤ºæ•°æ®"æŠ€æœ¯èƒ½åŠ›å¾…æå‡"
        tech_skills = tech_breadth.split(',') if tech_breadth else []
        if len(tech_skills) < 5:
            risk_tags.append("æŠ€æœ¯å¹¿åº¦å¾…æå‡")
        
        # åˆ›æ–°æ„è¯†è¯„ä¼° - åŸºäºæ¼”ç¤ºæ•°æ®"åˆ›æ–°æ„è¯†ä¸€èˆ¬"
        innovation_indicators = data.get("èƒ½åŠ›ç‰¹å¾_åˆ›æ–°èƒ½åŠ›", "")
        if not any(keyword in innovation_indicators for keyword in ['åˆ›æ–°', 'æ”¹è¿›', 'ä¼˜åŒ–', 'çªç ´']):
            risk_tags.append("åˆ›æ–°æ„è¯†ä¸€èˆ¬")
        
        # æ²Ÿé€šåä½œé£é™©
        collab_ability = data.get("èƒ½åŠ›ç‰¹å¾_æ²Ÿé€šåä½œ", "")
        if not any(keyword in collab_ability for keyword in ['åä½œ', 'æ²Ÿé€š', 'å›¢é˜Ÿ', 'åˆä½œ']):
            risk_tags.append("åä½œèƒ½åŠ›å¾…è§‚å¯Ÿ")
        
        # å¦‚æœæ²¡æœ‰æ˜æ˜¾é£é™©
        if not risk_tags:
            risk_tags.append("æ— æ˜æ˜¾é£é™©")
        
        return risk_tags[:2]  # æœ€å¤š2ä¸ªé£é™©æ ‡ç­¾

    def _generate_final_excel_format(self, structured_data: dict, reasoning_results: dict) -> dict:
        """ç”Ÿæˆæœ€ç»ˆExcelæ ¼å¼æ•°æ®"""
        
        # ç”Ÿæˆå‘˜å·¥å·¥å·
        timestamp = str(int(datetime.now().timestamp()))[-6:]
        employee_id = f"r{timestamp}"
        
        # åŸºç¡€ä¿¡æ¯å¤„ç†
        name = structured_data.get("åŸºç¡€ä¿¡æ¯_å§“å", "")
        gender = structured_data.get("åŸºç¡€ä¿¡æ¯_æ€§åˆ«", "")
        age = structured_data.get("åŸºç¡€ä¿¡æ¯_å¹´é¾„", "")
        phone = structured_data.get("åŸºç¡€ä¿¡æ¯_è”ç³»æ–¹å¼_æ‰‹æœº", "")
        email = structured_data.get("åŸºç¡€ä¿¡æ¯_è”ç³»æ–¹å¼_é‚®ç®±", "")
        
        # æ•™è‚²èƒŒæ™¯
        school = structured_data.get("æ•™è‚²èƒŒæ™¯_æ¯•ä¸šé™¢æ ¡", "")
        education = structured_data.get("æ•™è‚²èƒŒæ™¯_æœ€é«˜å­¦å†", "")
        
        # å·¥ä½œä¿¡æ¯
        position = structured_data.get("å·¥ä½œç»å†_å½“å‰èŒä½", "")
        work_years = structured_data.get("å·¥ä½œç»å†_å·¥ä½œå¹´é™", "")
        
        # å¤„ç†æ•°æ®
        birth_date = self._calculate_birth_date(age)
        masked_phone = self._mask_phone(phone)
        years_num = self._extract_years(work_years)
        job_level = self._infer_job_level(position, years_num)
        work_start_date = self._estimate_work_start_date(years_num)
        
        # ç»„è£…æ¨ç†ç»“æœ
        tech_tags = ";".join(reasoning_results['tech_capabilities'])
        mgmt_tags = ";".join(reasoning_results['mgmt_capabilities'])
        business_tags = ";".join(reasoning_results['business_capabilities'])
        potential_tags = ";".join(reasoning_results['potential_assessment'])
        risk_tags = ";".join(reasoning_results['risk_assessment'])
        
        excel_data = {
            "å‘˜å·¥å·¥å·": employee_id,
            "å§“å": name,
            "æ‰€å±ç»„ç»‡": "æŠ€æœ¯ç ”å‘éƒ¨",
            "æ€§åˆ«": gender,
            "å‡ºç”Ÿæ—¥æœŸ": birth_date,
            "èº«ä»½è¯": "",
            "æ‰‹æœºå·": masked_phone,
            "é‚®ç®±": email,
            "æ¯•ä¸šé™¢æ ¡": school,
            "æœ€é«˜å­¦å†": education,
            "æ‹…ä»»å²—ä½": position,
            "èŒçº§": job_level,
            "å‚åŠ å·¥ä½œæ—¶é—´": work_start_date,
            "å…¥å¸æ—¥æœŸ": "",
            "å·¥ä½œç»éªŒ(å¹´)": years_num,
            "ç»©æ•ˆç­‰çº§": "",
            "èŒä¸šèµ„è´¨": "",
            "æŠ€æœ¯èƒ½åŠ›æ ‡ç­¾": tech_tags,
            "ç®¡ç†èƒ½åŠ›æ ‡ç­¾": mgmt_tags,
            "ä¸šåŠ¡èƒ½åŠ›æ ‡ç­¾": business_tags,
            "æ½œåŠ›æ ‡ç­¾": potential_tags,
            "é£é™©æ ‡ç­¾": risk_tags
        }
        
        return excel_data

    def _extract_team_size(self, text: str) -> int:
        """æå–å›¢é˜Ÿè§„æ¨¡"""
        # æŸ¥æ‰¾æ•°å­—+äººçš„æ¨¡å¼
        import re
        patterns = [r'(\d+)äºº', r'(\d+)ä¸ªäºº', r'å¸¦é¢†(\d+)', r'ç®¡ç†(\d+)']
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return int(match.group(1))
        
        return 0

    def _extract_years(self, years_str: str) -> int:
        """æå–å¹´æ•°"""
        if not years_str:
            return 5
        
        import re
        match = re.search(r'(\d+)', years_str)
        if match:
            return int(match.group(1))
        
        return 5

    def _calculate_birth_date(self, age_str: str) -> str:
        """è®¡ç®—å‡ºç”Ÿæ—¥æœŸ"""
        if not age_str:
            return ""
        
        try:
            import re
            age_match = re.search(r'(\d+)', age_str)
            if age_match:
                age = int(age_match.group(1))
                birth_year = datetime.now().year - age
                return f"{birth_year}-01-01"
        except:
            pass
        
        return ""

    def _mask_phone(self, phone: str) -> str:
        """æ‰‹æœºå·è„±æ•"""
        if not phone or len(phone) < 7:
            return phone
        
        import re
        phone_digits = re.sub(r'\D', '', phone)
        
        if len(phone_digits) >= 11:
            return phone_digits[:3] + "****" + phone_digits[-4:]
        
        return phone

    def _infer_job_level(self, position: str, years: int) -> str:
        """æ¨æ–­èŒçº§"""
        if "æ€»ç›‘" in position or "VP" in position:
            return "P8-æ€»ç›‘çº§"
        elif "æ¶æ„å¸ˆ" in position:
            return "P7-ä¸“å®¶çº§"
        elif "é«˜çº§" in position:
            return "P6-é«˜çº§çº§"
        elif "ç»ç†" in position:
            return "M6-ç»ç†çº§"
        else:
            return "P5-ä¸­çº§"

    def _estimate_work_start_date(self, years: int) -> str:
        """ä¼°ç®—å·¥ä½œå¼€å§‹æ—¶é—´"""
        start_year = datetime.now().year - years
        return f"{start_year}-07-01"

    def _call_api(self, text: str, schema: dict, examples: list, system_prompt: str):
        """è°ƒç”¨API"""
        
        deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
        if deepseek_api_key:
            try:
                print("ä½¿ç”¨ DeepSeek API è¿›è¡Œé«˜çº§æ¨ç†åˆ†æ...")
                
                model = OpenAILanguageModel(
                    model_id="deepseek-chat",
                    api_key=deepseek_api_key,
                    base_url="https://api.deepseek.com/v1",
                    system_prompt=system_prompt
                )
                
                result = lx.extract(
                    text,
                    schema,
                    examples=examples,
                    model=model
                )
                
                print("âœ“ DeepSeek API é«˜çº§æ¨ç†åˆ†ææˆåŠŸ")
                return result
                
            except Exception as e:
                print(f"âœ— DeepSeek API å¤±è´¥: {e}")
                raise
        
        raise ValueError("æ²¡æœ‰å¯ç”¨çš„ DeepSeek API key")


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python advanced_reasoning_system.py <æ–‡æœ¬æ–‡ä»¶è·¯å¾„>")
        sys.exit(1)
    
    text_file = sys.argv[1]
    
    if not os.path.exists(text_file):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {text_file}")
        sys.exit(1)
    
    try:
        # åˆ›å»ºé«˜çº§æ¨ç†ç³»ç»Ÿ
        reasoning_system = AdvancedReasoningSystem()
        
        # æ‰§è¡Œé«˜çº§æ¨ç†åˆ†æ
        excel_data = reasoning_system.analyze_resume_with_advanced_reasoning(text_file)
        
        # ä¿å­˜ç»“æœ
        base_name = os.path.splitext(os.path.basename(text_file))[0].replace("_extracted", "")
        output_file = f"outs/{base_name}_advanced_reasoning.json"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs("outs", exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(excel_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ é«˜çº§æ¨ç†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ˜¾ç¤ºç»“æœé¢„è§ˆ
        print("\n=== é«˜çº§æ¨ç†åˆ†æç»“æœ ===")
        for key, value in excel_data.items():
            print(f"{key}: {value}")
        
        # æ˜¾ç¤ºæ¨ç†æ ‡ç­¾å¯¹æ¯”
        print("\n=== æ¨ç†æ ‡ç­¾åˆ†æ ===")
        print(f"ğŸ”§ æŠ€æœ¯èƒ½åŠ›: {excel_data.get('æŠ€æœ¯èƒ½åŠ›æ ‡ç­¾', '')}")
        print(f"ğŸ‘¥ ç®¡ç†èƒ½åŠ›: {excel_data.get('ç®¡ç†èƒ½åŠ›æ ‡ç­¾', '')}")
        print(f"ğŸ’¼ ä¸šåŠ¡èƒ½åŠ›: {excel_data.get('ä¸šåŠ¡èƒ½åŠ›æ ‡ç­¾', '')}")
        print(f"ğŸš€ å‘å±•æ½œåŠ›: {excel_data.get('æ½œåŠ›æ ‡ç­¾', '')}")
        print(f"âš ï¸  é£é™©è¯„ä¼°: {excel_data.get('é£é™©æ ‡ç­¾', '')}")
        
    except Exception as e:
        print(f"\nâœ— é«˜çº§æ¨ç†åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()